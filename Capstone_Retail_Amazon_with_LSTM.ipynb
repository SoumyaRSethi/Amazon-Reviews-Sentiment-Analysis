{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RANZ1XMqF6e4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZv7fHqE4_yA"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOXXCLA54gs5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eB_oz0FiF6fN"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "test_predection = pd.read_csv('test_data_hidden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "EhrMrEElF6fZ",
    "outputId": "1b81491f-bf78-43f5-d20a-523a180092ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>primaryCategories</th>\n",
       "      <th>reviews.date</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8\" HD Display, Wi-Fi...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2016-12-26T00:00:00.000Z</td>\n",
       "      <td>Purchased on Black FridayPros - Great Price (e...</td>\n",
       "      <td>Powerful tablet</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon - Echo Plus w/ Built-In Hub - Silver</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Echo,Smart Home,Networking,Home &amp; Tools...</td>\n",
       "      <td>Electronics,Hardware</td>\n",
       "      <td>2018-01-17T00:00:00.000Z</td>\n",
       "      <td>I purchased two Amazon in Echo Plus and two do...</td>\n",
       "      <td>Amazon Echo Plus AWESOME</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Echo Show Alexa-enabled Bluetooth Speak...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Echo,Virtual Assistant Speakers,Electro...</td>\n",
       "      <td>Electronics,Hardware</td>\n",
       "      <td>2017-12-20T00:00:00.000Z</td>\n",
       "      <td>Just an average Alexa option. Does show a few ...</td>\n",
       "      <td>Average</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire HD 10 Tablet, 10.1 HD Display, Wi-Fi, 16 ...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>eBook Readers,Fire Tablets,Electronics Feature...</td>\n",
       "      <td>Office Supplies,Electronics</td>\n",
       "      <td>2017-08-04T00:00:00.000Z</td>\n",
       "      <td>very good product. Exactly what I wanted, and ...</td>\n",
       "      <td>Greattttttt</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brand New Amazon Kindle Fire 16gb 7\" Ips Displ...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Computers/Tablets &amp; Networking,Tablets &amp; eBook...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2017-01-23T00:00:00.000Z</td>\n",
       "      <td>This is the 3rd one I've purchased. I've bough...</td>\n",
       "      <td>Very durable!</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  ... sentiment\n",
       "0  All-New Fire HD 8 Tablet, 8\" HD Display, Wi-Fi...  ...  Positive\n",
       "1        Amazon - Echo Plus w/ Built-In Hub - Silver  ...  Positive\n",
       "2  Amazon Echo Show Alexa-enabled Bluetooth Speak...  ...   Neutral\n",
       "3  Fire HD 10 Tablet, 10.1 HD Display, Wi-Fi, 16 ...  ...  Positive\n",
       "4  Brand New Amazon Kindle Fire 16gb 7\" Ips Displ...  ...  Positive\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YWcioFaF6fn"
   },
   "outputs": [],
   "source": [
    "Positive = data[data['sentiment']== \"Positive\"].iloc[:,[5,6,7]]\n",
    "Neutral = data[data['sentiment']== \"Neutral\"].iloc[:,[5,6,7]]\n",
    "Negative = data[data['sentiment']== \"Negative\"].iloc[:,[5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2QYvXYMVF6fy",
    "outputId": "d4460552-4d97-4ba0-e34a-ca960b180bf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3749\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PwJMY7h1F6f-",
    "outputId": "dd313dab-a295-4569-94f0-dfa69e00cda8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral    158\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neutral['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rPJXsjKzF6gI",
    "outputId": "302d6fcf-e5d2-424d-8127-820bc17a99cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    93\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VofcnivxF6gU"
   },
   "outputs": [],
   "source": [
    "# 2.Convert the reviews in Tf-Idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMWLe8DRF6ge"
   },
   "outputs": [],
   "source": [
    "# Keeping only those features that we will explore\n",
    "data1 = data [[\"sentiment\",\"reviews.text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "iLjE-udWF6gr",
    "outputId": "22f3c907-5cce-49b5-dca4-1221cbebdaf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Purchased on Black FridayPros - Great Price (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I purchased two Amazon in Echo Plus and two do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Just an average Alexa option. Does show a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>very good product. Exactly what I wanted, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This is the 3rd one I've purchased. I've bough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                       reviews.text\n",
       "0  Positive  Purchased on Black FridayPros - Great Price (e...\n",
       "1  Positive  I purchased two Amazon in Echo Plus and two do...\n",
       "2   Neutral  Just an average Alexa option. Does show a few ...\n",
       "3  Positive  very good product. Exactly what I wanted, and ...\n",
       "4  Positive  This is the 3rd one I've purchased. I've bough..."
      ]
     },
     "execution_count": 170,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkW1oW7eF6g4"
   },
   "outputs": [],
   "source": [
    "# Resetting the index\n",
    "data1.index = pd.Series(list(range(data1.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "p8PUxnjYF6hD",
    "outputId": "2c7bfe0f-2861-473f-bcd0-2ba4c1ee3d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  (4000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Purchased on Black FridayPros - Great Price (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I purchased two Amazon in Echo Plus and two do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Just an average Alexa option. Does show a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>very good product. Exactly what I wanted, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This is the 3rd one I've purchased. I've bough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                       reviews.text\n",
       "0  Positive  Purchased on Black FridayPros - Great Price (e...\n",
       "1  Positive  I purchased two Amazon in Echo Plus and two do...\n",
       "2   Neutral  Just an average Alexa option. Does show a few ...\n",
       "3  Positive  very good product. Exactly what I wanted, and ...\n",
       "4  Positive  This is the 3rd one I've purchased. I've bough..."
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape : ',data1.shape)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NJFFVoaF6hW"
   },
   "source": [
    "##Creating Preprocessing Function and Applying it on Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "W3VSdEbrF6hZ",
    "outputId": "9e3fd4dc-7167-488c-a9b6-37e0d46e53d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "#Download Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(document):\n",
    "    document = document.lower() # Convert to lowercase\n",
    "    words = tokenizer.tokenize(document) # Tokenize\n",
    "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
    "    # Lemmatizing\n",
    "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
    "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "adL3EZdcF6hi",
    "outputId": "66240ff4-e184-4a32-b547-c290423ee28f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>Processed_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Purchased on Black FridayPros - Great Price (e...</td>\n",
       "      <td>purchase black fridaypros great price even sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I purchased two Amazon in Echo Plus and two do...</td>\n",
       "      <td>purchase two amazon echo plus two dot plus fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Just an average Alexa option. Does show a few ...</td>\n",
       "      <td>average alexa option show thing screen still l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>very good product. Exactly what I wanted, and ...</td>\n",
       "      <td>good product exactly want good price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This is the 3rd one I've purchased. I've bough...</td>\n",
       "      <td>rd one purchase buy one niece case compare one...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  ...                                   Processed_Review\n",
       "0  Positive  ...  purchase black fridaypros great price even sal...\n",
       "1  Positive  ...  purchase two amazon echo plus two dot plus fou...\n",
       "2   Neutral  ...  average alexa option show thing screen still l...\n",
       "3  Positive  ...               good product exactly want good price\n",
       "4  Positive  ...  rd one purchase buy one niece case compare one...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['Processed_Review'] = data1['reviews.text'].apply(preprocess)\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TO3VS7F7F6hr",
    "outputId": "7513c039-3a69-4317-d08b-de264ddabafa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Processed_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>purchase black fridaypros great price even sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>purchase two amazon echo plus two dot plus fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>average alexa option show thing screen still l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>good product exactly want good price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>rd one purchase buy one niece case compare one...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                   Processed_Review\n",
       "0  Positive  purchase black fridaypros great price even sal...\n",
       "1  Positive  purchase two amazon echo plus two dot plus fou...\n",
       "2   Neutral  average alexa option show thing screen still l...\n",
       "3  Positive               good product exactly want good price\n",
       "4  Positive  rd one purchase buy one niece case compare one..."
      ]
     },
     "execution_count": 175,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data1 [[\"sentiment\",\"Processed_Review\"]]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpNTm7TOF6hz"
   },
   "outputs": [],
   "source": [
    "# Creating TF-IDF Matrix & multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTbk0_ptF6h5"
   },
   "outputs": [],
   "source": [
    "def textPreprocessing(data2):\n",
    "    #Remove Punctuation Logic\n",
    "    import string\n",
    "    removePunctuation = [char for char in data2 if char not in string.punctuation]\n",
    "    #Join Chars to form sentences\n",
    "    sentenceWithoutPunctuations = ''.join(removePunctuation)\n",
    "    words = sentenceWithoutPunctuations.split()\n",
    "    #StopwordRemoval\n",
    "    from nltk.corpus import stopwords\n",
    "    removeStopwords = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return removeStopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "x9WuaW1xF6iC",
    "outputId": "9272cb81-b203-4fdc-8805-f54053477900"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Processed_Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>93</td>\n",
       "      <td>78</td>\n",
       "      <td>last model kindle hdx terrible purchase model ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>158</td>\n",
       "      <td>145</td>\n",
       "      <td>sleek design color available small kid good ta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>3749</td>\n",
       "      <td>3372</td>\n",
       "      <td>give grandkids age christmas love</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Processed_Review  ...     \n",
       "                     count  ... freq\n",
       "sentiment                   ...     \n",
       "Negative                93  ...    3\n",
       "Neutral                158  ...    2\n",
       "Positive              3749  ...    4\n",
       "\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "B3sDose6F6iK",
    "outputId": "539cdb48-5bae-4a3a-aae0-315ecb9eb7c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [purchase, black, fridaypros, great, price, ev...\n",
       "1    [purchase, two, amazon, echo, plus, two, dot, ...\n",
       "Name: Processed_Review, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text preprocessing\n",
    "data2['Processed_Review'].head(2).apply(textPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcdYQd_pF6iY"
   },
   "outputs": [],
   "source": [
    "#My intention is to create Bag of Words\n",
    "#sklearn package CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(analyzer=textPreprocessing).fit(data2['Processed_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B24M3IngF6it"
   },
   "outputs": [],
   "source": [
    "#bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bSekLikNF6i5",
    "outputId": "29133809-b5f5-4137-f06f-725d7884cf93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 182,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xjp6zS3EF6jB"
   },
   "outputs": [],
   "source": [
    "reviews_bow = bow.transform(data2['Processed_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy7NguSEF6jI"
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfData = TfidfTransformer().fit(reviews_bow)\n",
    "tfidfDataFinal = tfidfData.transform(reviews_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6WjiQrymF6jR",
    "outputId": "80300d18-682e-4183-a637-ec6199197eba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3407)"
      ]
     },
     "execution_count": 185,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfDataFinal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcOnCdcFF6jc"
   },
   "outputs": [],
   "source": [
    "# The data is reddy for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANqblAHTF6jj"
   },
   "outputs": [],
   "source": [
    "#Training the model --- NaiveBayes Algo\n",
    "#Handling String data ---- MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(tfidfDataFinal,data2['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "binD5cgiF6jp",
    "outputId": "314679f5-419d-4219-cda0-152094bbbce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 188,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oWE5WH7yF6jx",
    "outputId": "8782ed82-c669-4c54-a0fc-fa1453420810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 189,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData = 'dont like very bad'\n",
    "l1 = textPreprocessing(inputData)\n",
    "l2 = bow.transform(l1)\n",
    "l3 = tfidfData.transform(l2)\n",
    "prediction = model.predict(l3[0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G18BijgVF6j2"
   },
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGPofvV8F6j4"
   },
   "outputs": [],
   "source": [
    "#Tackling Class Imbalance Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eEE8MX80F6kB",
    "outputId": "e920d308-9af6-47b3-cd59-279537e8c3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "#Create independent and Dependent Features\n",
    "columns = data2.columns.tolist()\n",
    "# Filter the columns to remove data we do not want \n",
    "columns = [c for c in columns if c not in [\"sentiment\"]]\n",
    "# Store the variable we are predicting \n",
    "target = \"sentiment\"\n",
    "# Define a random state \n",
    "state = np.random.RandomState(42)\n",
    "X = data2[columns]\n",
    "Y = data2[target]\n",
    "# Print the shapes of X & Y\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Jy4E4cNGF6kK",
    "outputId": "b996962b-4752-4286-99de-fad81f52cceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive    3749\n",
      "Neutral      158\n",
      "Negative      93\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data2.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "rMVkJkIHF6kS",
    "outputId": "757f6b2c-3f5f-4638-95b1-055e5c8101c9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEaCAYAAACimQj6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFoKGiJiEg0FLRERCQaCloiIhKN/wMkrSAQ0uduPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# using seaborns countplot to show distribution of questions in dataset\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(data1.sentiment, palette='viridis')\n",
    "g.set_xticklabels(['Positive', 'Neutral','Negative'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "# function to show values on bars\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "show_values_on_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Transactions', fontsize=30)\n",
    "plt.tick_params(axis='x', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6SYvRSd1F6kY",
    "outputId": "94c90244-44a0-4904-a3d5-1c1d96abb576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3749, 3) (158, 3) (93, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Positive.shape,Neutral.shape,Negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4hqdiT3F6ke"
   },
   "outputs": [],
   "source": [
    "## Applying Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ch-T9JAuF6km",
    "outputId": "d562cef6-b9e1-4d37-d930-0f1e8718fd4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## RandomOverSampler to handle imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0) \n",
    "X_res,Y_res=ros.fit_sample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Dl7P9hJF6k7",
    "outputId": "e38f07d1-97ac-4f0a-b94c-dde51d6d9a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Negative', 3749), ('Neutral', 3749), ('Positive', 3749)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(Y_res).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7g0nxSeFF6lE",
    "outputId": "a2e57c9d-7ec6-43f8-e192-aca55a7af510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11247, 1), (11247,))"
      ]
     },
     "execution_count": 198,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape,Y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l0CVN2AUF6lN",
    "outputId": "ad7bd464-6e9a-4f98-d912-27af02beccb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'Positive': 3749, 'Neutral': 158, 'Negative': 93})\n",
      "Resampled dataset shape Counter({'Positive': 3749, 'Neutral': 3749, 'Negative': 3749})\n"
     ]
    }
   ],
   "source": [
    "#Checking out old and new data\n",
    "print('Original dataset shape {}'.format(Counter(Y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(Y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha57QhxBF6lT"
   },
   "outputs": [],
   "source": [
    "#Creating X output to dataframe\n",
    "X1=pd.DataFrame(X_res,columns=['Processed_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MvSR-AbqF6la"
   },
   "outputs": [],
   "source": [
    "#creating Y output to dataframe cor merge\n",
    "Y1=pd.DataFrame(Y_res,columns=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "OKiKZ1Q7F6lg",
    "outputId": "e0baadd9-847f-49fb-e35f-a027d1acede6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>purchase black fridaypros great price even sal...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchase two amazon echo plus two dot plus fou...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>average alexa option show thing screen still l...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good product exactly want good price</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rd one purchase buy one niece case compare one...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Processed_Review sentiment\n",
       "0  purchase black fridaypros great price even sal...  Positive\n",
       "1  purchase two amazon echo plus two dot plus fou...  Positive\n",
       "2  average alexa option show thing screen still l...   Neutral\n",
       "3               good product exactly want good price  Positive\n",
       "4  rd one purchase buy one niece case compare one...  Positive"
      ]
     },
     "execution_count": 202,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge the X & Y output to final data\n",
    "Final_data=pd.concat([X1,Y1],axis=1)\n",
    "Final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "jA24ZfmHF6ls",
    "outputId": "bd35d841-a65c-45f5-943f-2db85f0d466a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11247 entries, 0 to 11246\n",
      "Data columns (total 2 columns):\n",
      "Processed_Review    11247 non-null object\n",
      "sentiment           11247 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 175.9+ KB\n"
     ]
    }
   ],
   "source": [
    "Final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "iJE1puIPF6l6",
    "outputId": "e2314173-adb6-4ea3-de9f-b3422bde4b13"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEaCAYAAACimQj6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de7xVVbn/8c8jOy/hLRK8B6aYBSql\nkhoqKtJFj6KeMiuNbmR6Muzm6Soe6/x+ar/KYyd/7TSpV2Z4gxLTvIHHShMyLLXMNBGvgIhggG7g\nOX88c7HmXqzb3ntd9mB/36/Xeq3LHHPMMeeYcz5zjjnmXObuiIiIpGCzdhdARESkXgpaIiKSDAUt\nERFJhoKWiIgkQ0FLRESSoaAlIiLJ6BdBy8w8e81td1l6w8ym5+ZhRJnh43PDp7W8gA2yqcxHq5jZ\n7mZ2iZk9bGYv55bdgnaXTdJjZtvn1qFZ7S5Pu3TUm9DMKt3Q9SqwAngJWAjcD/weuMndV/e5hA2Q\n28E+4e7T21iUfsXMtgemZl8XuPuA3RAazcz2AX4HvK6X408GrmxQcfZw9ycalJc0kJn9O7Al8Jy7\n//92lycFdQetKjYHdsheewJHZb8vN7MfA+e5+0sNmE5fnJe93wVMb2M5+pvtKS6bHwMKWo1zEcWA\nNRv4BbA0+97u7UH6j38HtgMeABS06tDboHVi7rMRC/11wBjgcGAEsUP8DHCymZ3q7r+plJm7Wy/L\n0S+4+2RgcpuL0XTuPpeob6nCzF4DHJN9/QtwvPf80TN30n07K3U2cGT2+dIsfSWLezht6YfcfTna\n/noXtKo1I5mZAe8GvguMBHYDZpvZO9z9oV6VUiQtOxBNPgAP9CJg4e5PAk9WGm5mk3Jf71fTrgwU\nDe+I4eFXwIFA4exqO+BaM+sXHT9EmmyL3OdX2lYKkU2Ru9f1Arzw6sE4OwMv5sY9tUbec6vktQvw\nH8A9wDKgK8v7UeBu4NvAYZXKXOM1PjfO+Nzv07Lf3kScOf6F6HTiwOTcONNz44woU/Zyee4LdAKP\nAauBJcDtlZZRLq/Jubwm9yYt0Xxb77IZUW0+qkz7tcA5wBzgOWLnvZg4kPkSsF2N8aeV1g9wMHAV\n0eGnkN9s4F31rpN1rrdDgK9l69oSorPRs1n9fBrYssJ4+fWgrmXahzLmp1V1PciNMyk3ztTcevh9\n4BHg5WzYpJLxDgbOB24DngLWAKuyergOeC+wWY1pT81Ne1L221uJzib/yPJcCtxSOv0K+W2drV93\nAs9ndbQyy+v3wA+AE4BBFcbv8zyV5DcU+AowN1tXXsmW58PAT7L8Ns+lX17nujIpN872ud9n1SjP\nIOA04lpqYf6WAX8krrcO78W6shfRFP0osc96kegn8NE66v81wMeBm4Gns+WzimhN+EO2Pn+ACttW\nt7x6UCk9DlrZeBfmxr2tRt5zKww/Nlsha1Xw8kplrvEanxtnfO73acDp2cItHWdybpzpud832iGV\nyfO0bCWqVJ7ZlSqPBIIWsUN4uka+S4GJVfKYlq8fYoewrkp+5/dkvawy3RPofqBV7rUQeGuZcafX\nGK/iOtKLcuanVXU9yI3TbUcEnEXs7KvtKL9T5zz9D7BDlWl3C1pE8O+qkt8lVfIaBSzq7bJu1Dzl\n8vs3igG/2uszuXGaFrSANxDBqVq+q4GzerCu/GuNebyBygcIuwJ/rnN+x9da3o3oPVjLz4AvZp8P\nNbPXuHtXvSOb2a7Az4kjK4CbiKOjZ4jmzWHA/sSF7+1KRi9cyJ6ZvT8EfLXMZB6sMPl3UNxZXgH8\nlgg2byLOHnrjIODL2ecfERvGuuz3jwGDiSD9U2JFaYbFxLIZRhyRQpwR/VeFtHUzs7cSR79bZT/9\nkVgHngR2At5HLNfXE9c6J3p08KhmCnAqEQinE/W4OfAu4BTi4vTXzewud6/WIaFW2d8DXE8cpULU\nzXXEkfxw4mBjX2KncJeZjXX3v+ay+C+iB2at5dofOka8C5hIHJBdTpyddBEB4YVcuq2y3+8G7gX+\nTuy8Cr2FTyPm9zDgGjOb4O7ra0z7FOD9xHK4EvgTscyPzvLbDDg7q88b8iNmnVxmEtfKyco9C3iC\n2I6GAG8hejGPrjD9hs2TmV1A933KHGIf9RRxdrEncdB1ON07UXwwG34V0SrxBHHmWOr3FeahLDPb\ngWjN2D37aSGxzfwV2AZ4DxGQtgS+Z2bu7t+vke044Hhi3/c94D5gLXAo8AmiOfxEouPdt8uM/1OK\ndfEwcA1xtvYqsc/eh1hGB9Q1kz04stsQDXt4RDiI7hF6/yp5zy0z7PO54V+sMh2jpHmwnvzLpB1P\n98j/LPCWGuNMz6Uvd2RXmucK4OAy6UbS/Qzl5DJpJueGVz3CrpWW7mdc03u4bKaVGb4ZcQBQSPNd\nyjQbEE1vhTRPUuasku5nWg7cCgwuk+6cXJpf9WTdLMlnWyI4FfL6bJk0HUSTbiHNvAp59Wi59rK8\n+XWu6nqQGyd/9OxEU9oba4xzCPD6KsO3JA6+CnmeUCHd1JJp/xbYvky6j+TS/K7M8Am54VcBVqVs\nY4BtmjhP7wTWZ2lWAsdWyXME5c/OC2dcC+qov5pnWsDVuTQ3V9hmTqJ4lrsa2LOOdeWvwO5l0h2d\nWwaLKNneiSbFQh5zgC2qzN9IYOday6HpHSPcfR1x1FEwtIdZ7JX7/MMq03F3v7uHedfjk+7+cIPz\n/IK731v6o7s/SpxtFXy+wdNttuOII3WII9hzvMwRqrtfQByNQhwRfqhGvi8Ap7j7P8sMu4RiL7uj\nzKy3rQeTiaNrgGvcfaMjRndfC3yKODMAONDMJvRyev3Bh9398WoJ3P0ed3+hyvA1xDJZkv10Wh3T\n/Sfwrx5duEvzu5Jiy8fBZrZtSZL8/uByz/Z2Fcq2wN1Xlvm9UfN0AcWzp4+5+00V0uHuT7j7HysN\nbwQz24NoyYA4iz213DbjcfZ6YfZ1S8qf4XUbhdj+FpXJ6w7gV9nX3Shu/wX5+vqJu1fsmOTuj7r7\nszXK0rLHOL2Y+/z6Ho67Kve5dIE020Lgxgbn+SJVnnTg7rcQp9AQG+1ODZ5+M52U+3xxtR0K8H8r\njFfOT9z9xXIDsqB4V/Z1C6I5pjfyZbiwUqLsIOziCuOl5E/u/j+NyCjbEf0h+/r2Oka5psbOaU72\nbsCbS4a1ZH9Qa57MbCTRpA9xlnRNs8rSAydQ3Kf/oNxBQc53iOY5qH4/IEQL1QNVhueb5N9SMqzh\n9dWqoJWfTrUdWTm35T7fYGbnmNluFVM31m9q7Hh74253f7VGmvxKcFDFVP3P2Ozd6V5v5fyOaDaG\n2ju6jc5KSzyd+9zjxyZl9xYWlvNSd7+/xii35j7Xs5Puj+pulTCzDjM7xcx+bmaPmNlLZrYu9xw8\nJ66RAeySLc9q+lKfc4jrKQAXm9l/mllpYKupAfM0Lvf5lz2dfpOMzX2+tWIqIDvTLATlXcxs9yrJ\n+1Jf8ymetJxjZv9tZgfUsY5U1KqgtX3u87KejOjuNxMX8iGaFr8NLDKzv2UPqv2omQ2rnEOfPF07\nSY/9vYdpdmlCGZpl5+z9uXLNMnnZGdJj2dchZrZ5leRLqwyD7vdCbVkxVWXbEhfDIS4QV+Xuiyk+\nimnnamn7sbrWbTPbC1hAdIY6BdibWF6V9h2bUew0VUmv6zNrovpKbtiXgIfN7Bkzu9bMzs7KXFGD\n5il/4PyXatNrofy6+Lc60ufTVFuP+1Jfq4hequuI5XgmEciWmNmNZvZFM9uvjrJu0PSgZWaD6F7B\nSyqlreJDRB///BM1RgIfJnr1PWNmPzOzRu9AmvHA31W1k5Bvh661A+hPtsney117Kufl3OdtKqaK\nC73NlJ92T8terdz9Wc1128wGE/emFZp1niU6okwl7qk5iWhaOpHoUVYwiOr6VJ/ufhHx1J27Kbbc\n7Ez0tr0EeNTM7ii3M2zgPOWvtb1M/9DT9bgl25+7X030GL6ZCF4Ql4mOI5riHzCz+WZ2RD35taLL\n+74Uj2L/SffAU5esie4K4AozeyNxan4o0a11JLFCnQqMM7OD3P35RhS8SV5bOwmDc5/7skG0+gkk\nK4mz6sG1EmbyAbnqmVmT5afd07K3s9zN9jGiqz9EN/NTK11IN7OzWlYqNlz7vSVrZTmM2B+MJ25Y\nNmLfcK+ZHe7u83OjNmqeVuQ+95cDy9L1uFbgatn25+6/B96T/bPEOKIH5xHEPZ2DiO7ud5rZJHev\n2o+gFTu1D+Q+/y7rgdVr7v64u//E3c9w972JmS30ytkd+EJf8m+Bqk0XZdI8UzIsv4FVa1KDuO+k\nlQoX13cys6obctamXeg08UId1/maaQXFM+Ca9WNmQyneE1haP5uSfM/Is6v1/KIYCFrK3Re7+/Xu\n/jl3PwB4I8VrTFvRvcMPNG6e8j2ie3xNrUnynVtG1pE+n6Yl67G7L3f32e7+FXcfR9x4XOgVvhnR\nQaSqpgatrLnuE7mfrmj0NLKL5vkuqePKJSsUqdHT74Vx2Q2S1RyZ+zyvZFi+R1Ct6121OgnkT/sb\nsWwKzSlG951DOYdSPNK7r1rCZsvO5AvLeaiZjakxysTc57aWvcl2zN7XuPtTlRJlrR/17CSbzuN/\nw06h2EJRuj9o1DzlO7Ic38Ni5hW2wUZuf1D8l4GyzGwI8XxYgGerLYtmylrFPknxWvKetXpMNy1o\nmdk2xJ3PhU4YfwGubdLknsh9LtfkWViB6236aaYhVPkbEzObSLG9/R53L33yRv6esaOoILtn419q\nlCXf9NiIZXN97vPna/QQOrfCeO2SL0PFs/XsGm3+/rn+UPZmKZx9bpk9maaSr7WiMPXK7rMqbDeD\nStbDhsyTu/+d4tMqxpjZ+6qlr6KR+6ZZFIPgJ8vc45Y3lWJLTVvX4eygcWHup6qXrRoetCy8m+gh\nUjjKWQG8t9yNpnXk93UzO6bGE+LPzH0udz/BP7L3fcxsqzLDW+1bZrZRV3Yz25O4E7/g/5WmcfeF\nFHsrHWZmGwWmrPnqOuIxMRW5+zKKveDG9KUbauYmitcs30F0Sd6o3szsyxQD6iLiyQbtNp3i45U+\nYGZnlybIAtb3iCctQDwR447WFK8t8mf53yy3fpjZ52nhf8mZ2cfN7ANmtkWVNMdQbOb9U8ltK42c\np69TbMW5wsyOrVKmN1g84qxUYd80PDv76bXsLHNG9nUn4GdmttE1dDM7gfjzSYhHM323L9OtxsxO\nNLMzql0uMLN9KcaKJXRv5txIrzpiWPf/8jGi58kQin8CuUdu+FPExc7e/pfWUcTTmJ8zs18TXVWf\nIwLuLsSp+WFZ2lco/+yrO4D9iKOZGy3+UXkpxRXuvmwH3gq/Ik7df5uV4266P3uwULnXu3ulI6Bv\nUWxqvd7MCs8wNOJC9EeIM9xriadLV1P4s8E9gRlmdgPdmyDvcve6elG6+3oz+xBxD9ZWwOeAI83s\nKmI92JG4Y7+wgnYBp2dHxm3l7ivN7CPE9ZBBwCVmdiIR/JcQzxs8jViPIC5cn96OsrbQD4jnyW1O\n9NTdx8xmEN3ldyXq8mCipWMRxe2wmUZnZbrMzG4lDo6fIu7d2pFoWj8ul/4/S8Zv2Dy5+61m9k3i\n2YNbE8/SvJPYxp8i9q97EB0OjiTO0EufinEHsc/sAH5pZpdTfJwYxH+l9eRZlWdnZd6NeIbpQ2Z2\nJfEU/22IXpf5G+I/5+6PbZRL4wwnrlN9x8zuIJownyB6rw4lOmScTLGr/IXZDfyV1XrOkxefC+U9\nfL1IdD/d6PliVfKeW2bYnDqnt4QKTw0nVsbFVcYdn0s7Pvf7tDqXzfTcOCPKDO+WJ9GFv9pT3m+i\nyiP6ieA0vcr4rxC3CEzO/Ta5Ql5jKP8U+43mp95lQ6yIz9SorxeAd1bJY1q5+ulr2jrq8gRqP4F7\nIfC2KnmMyKWd3pfy1LnOla3bMuNs9HcTdYzzQco/Cb7weowIJLNyv5V7puBGf01SZZoV01L/E9pX\nA2c2c55Kyru6jjKdXWbc1xOPIas0Tm+e8j6cOLivVpY19OAp771dr4gDhHrqay3wDao8S7LwakSX\n9y6i+W8FEUHvJ9p6Z3udR+g1HE9c1D+C6Cm4F1HRTtyo/BDR//8Kr/DYEnd/2szeRhzpHE3sVAbT\npo4Z7v5TM3uAOCo6ijhjXEWsaFe4+89qjO/ZWcEtREeXtxJd6Z8lzpy+6+5/NrPJdZRlgZkdAHyW\nOELbnfq65VfL8x6Lx9xMIYLAW4gNbgVxQ+Ns4PuV6qud3P0XWTPtmcSR6l7EPTkvEs/E+wXwwwat\n2/2eu19lZg8R1/mOIJ7P+BLwOPF3FJe5+4q+tyzX7Vxi/TmaaJ3YOytTB7F+PUKcvVzu0ZS+kUbP\nk7t/18yuAc4gOunsSazva4gDnHnEejO7zLgvmNmBxL5pItH7cWv6sG9y94XZNv0B4szxbURP4tXE\nPvpW4L8rLZ8Gu5SY/wlEx7B9iHvqNidaKx4jWoku9zqf8WpZNBQREen3Wn3zqYiISK8paImISDIU\ntEREJBkKWiIikoxWPDC32dSTRESkZ/rDI+16RWdaIiKSDAUtERFJhoKWiIgkQ0GrhdasWcPYsWPZ\nf//9GTVqFOeddx4Ahx12GGPGjGHMmDHssssuTJo0qdt48+bNo6Ojg+uuu27Db+eeey6jR49m9OjR\nzJgxA2k+1V/aVH+biGY8F63Fr2SsX7/eV65c6e7ur776qo8dO9bvueeebmlOOukk//GPf7zh+9q1\na/3II4/0d7/73X7ttde6u/vs2bN9woQJ3tXV5S+//LIfeOCB/tJLL7VuRgYo1V/aVH/dtHu/3euX\nzrRayMzYeut4iHtXVxddXV3kn2+2YsUK7rzzzm5Hepdeeiknn3wyw4YN2/Dbww8/zOGHH05HRweD\nBw9mv/3245ZbbmndjAxQqr+0qf42DQpaLbZu3TrGjBnDsGHDOOaYY3j724t/Ljxr1iyOPvpott02\n/rvt6aefZubMmXzqU5/qlsf+++/PLbfcwqpVq1i6dClz5sxh0aJFLZ2PgUr1lzbVX/o2hfu0kjJo\n0CAWLFjA8uXLOfHEE3nwwQcZPXo0AFdffTUf//jHN6SdOnUqF154IZtt1v3YYuLEicybN49DDz2U\noUOHcsghhzBo0KCWzsdApfpLm+pvE9Du9skGvJJ1/vnn+8UXX+zu7kuWLPEhQ4b46tWrNwwfMWKE\nDx8+3IcPH+6DBw/2oUOH+syZMzfK59RTT/WbbrqpZeWWoPpL2wCvv3bvt3v9ansBGvBKxuLFi/3F\nF190d/dVq1b5uHHj/MYbb3R398suu8xPP/30iuN++MMf3nAheO3atb506VJ3d3/ggQd81KhR3tXV\n1eTSi+ovbaq/btq93+71a1P4P626Z2DiKf/RzHLUtHL5czx038wNC3/H3Uex56jxAMyfcyUj9hnH\nDjuPLDvug/fNZOjOe7Pj7qNYt66L39/2AwA6OrbgzQccxzav27lVs1HVrTO+3pR8x3xjWlPy7YnV\nzz3Pk9fPAl8P7mw3ehQ7HXkEAH+/YjrDDh/HtiP3KjvukzfMYtu992b70W9hfdda/nZZ1N+gLbZg\nt+OPY6udd2rZfFSz4KvTmpLvp+/4TFPy7Ymljy/l9gtvxdfH9rfXESMZe1pc07rhs9dzwPsPYPjY\nEWXHvf2i2xhx8Aj2Onwka19dy4wzrgZg89duzvipRzF0r6Gtmo2KLj36kp4kT/YxTgpa0lCbctAa\nCDbloLWpGyhBS70HRUQkGQpaIiKSDAUtERFJhoKWiIgkQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgy\nFLRERCQZCloiIpIMBS0REUlGkkHLzKaY2Xwzm9/Z2dnu4oiISIsk+SeQ7t4JFKJV8k/8FRGR+iR5\npiUiIgOTgpaIiCRDQUtERJKhoCUiIslQ0BIRkWQoaImISDIUtEREJBkKWiIikgwFLRERSYaCloiI\nJENBS0REkqGgJSIiyVDQEhGRZChoiYhIMhS0REQkGQpaIiKSDAUtERFJhoKWiIgkQ0FLRESSoaAl\nIiLJUNASEZFkKGiJiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCQjyaBlZlPMbL6Zze/s7Gx3cURE\npEU62l2A3nD3TqAQrbydZRERkdZJ8kxLREQGJgUtERFJhoKWiIgkQ0FLRESSoaAlIiLJUNASEZFk\nKGiJiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCRDQUtERJKhoCUiIslQ0BIRkWQoaImISDIUtERE\nJBkKWiIikgwFLRERSYaCloiIJENBS0REkqGgJSIiyVDQEhGRZChoiYhIMhS0REQkGQpaIiKSDAUt\nERFJRpJBy8ymmNl8M5vf2dnZ7uKIiEiLdLS7AL3h7p1AIVp5O8siIiKtk+SZloiIDEwKWiIikgwF\nLRERSYaCloiIJENBS0REkqGgJSIiyVDQEhGRZChoiYhIMhS0REQkGQpaIiKSDAUtERFJhoKWiIgk\nQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCRDQUtERJKhoCUi\nIslQ0BIRkWQoaImISDIUtEREJBlJBi0zm2Jm881sfmdnZ7uLIyIiLdLR7gL0hrt3AoVo5e0si4iI\ntE6SZ1oiIjIwKWiJiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCRDQUtERJKhoCUiIslQ0BIRkWQo\naImISDIUtEREJBkKWiIikgwFLRERSYaCloiIJENBS0REkqGgJSIiyVDQEhGRZChoiYhIMhS0REQk\nGQpaIiKSDAUtERFJhoKWiIgkQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgykgxaZjbFzOab2fzOzs52\nF0dERFqko90F6A137wQK0crbWRYREWmdJM+0RERkYFLQEhGRZChoiYhIMhS0REQkGQpaIiKSDAUt\nERFJhoKWiIgkQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCRD\nQUtERJKhoCUiIslQ0BIRkWQoaImISDIUtEREJBkKWiIikgwFLRERSYaCloiIJENBS0REkqGgJSIi\nyVDQEhGRZCQZtMxsipnNN7P5nZ2d7S6OiIi0SEe7C9Ab7t4JFKKVt7MsIiLSOkmeaYmIyMCkoCUi\nIslQ0BIRkWQoaImISDIUtEREJBkKWiIikgwFLRERSYaCloiIJENBS0REkqGgJSIiyVDQEhGRZCho\niYhIMhS0REQkGQpaIiKSDAUtERFJhoKWiIgkQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgyFLRERCQZ\nCloiIpIMBS0REUmGgpaIiCRDQUtERJKRZNAysylmNt/M5nd2dra7OCIi0iId7S5Ab7h7J1CIVt7O\nsoiISOskeaYlIiIDk4KWiIgkQ0FLRESSoaAlIiLJUNASEZFkKGiJiEgyFLRERCQZCloiIpIMBS0R\nEUmGgpaIiCRDQUtERJKhoCUiIslQ0BIRkWQoaImISDIUtEREJBkKWiIikgwFLRERSYaCloiIJENB\nS0REkqGgJSIiyVDQEhGRZChoiYhIMhS0REQkGQpaIiKSDAUtERFJhoKWiIgkw9y93WXoMTObAkzJ\nvna6e2c7y9MsZjZlU523gUD1lzbVX/+UZNAaKMxsvrsf2O5ySO+o/tKm+uuf1DwoIiLJUNASEZFk\nKGj1b2pPT5vqL22qv35I17RERCQZOtMSEZFkKGiJiEgyFLR6yMymmZnnXs+Y2fVmtmeDp7E0933v\n7LftS9JNzsqwdaOmPRDk6vDXZYZdZ2ZzmzDN95nZ5AbnOT6bj9GNzLc/aEcd1UPbYvspaPXOS8Ah\n2evzwBjgDjMb3KD8Lwfemfu+N3AesH1JupuyMqxq0HQHmolmdlCLpvU+YHKLprUpaWUd1UPbYpt1\ntLsAiVrr7vdmn+81syeBu4H3ANf2NXN3fwp4qo50S4AlfZ3eALUMeBr4CjCpzWXZwMxeA6x393Xt\nLks/0C/rqBxti62jM63G+EP2PgI2NAX92cxeMbNFZvZNM9twgGBm25vZ5VnT4hoze9LMfpgbvqF5\n0MzGAzdmg/6RNUE8kQ3r1iRhZv8ws4tLC2dm15rZb3Lfh5hZp5k9n03/d2b29kYukAQ48E3geDPb\nt1IiM3uDmf3czJaZ2Soz+7WZvSk3vGwTnZnNNbPrss/TgZOBI3LNytPy6cxsipk9BqwBdjGzfbLp\nLsqm+5CZTTWzgbTNNqSOcmluNrPV2XYyubSZsdYy17bYPwykDaCZRmTvz5nZRGAGcD9wAnAp0YT4\nvVz6bwPjgHOIZsAvExtoOfdn4wOcRDRBnFgh7TXAe/M/ZBvRscDPs+9bALcDE4AvEEewS4DbzWyn\nmnO6abkWeJQ4kt+ImQ0BfgO8CTiDaOIbTCyrrXownQuAOcAfKTYrX54b/g7gU8C5wL8Qzc+7Ao8A\nZxJn8D8Ezs/SDCR9riMzM+CXwJuBjwKfBc4GSoNDrWWubbE/cHe9evACpgFLiabVDqKNew6wAtgZ\nuBeYUzLOF4F1wG7Z9weBT9eaRu77cURQG1GSbnL2+9bZ97dm3w/OpTkVWAvsmH3/GPAqMDKXpgN4\nDLi43cu3lXWYW4brgL2z79cBc7PPFwAvAENy476OCCpnZd/HZ8t8dMk05gLX5b5vyLdMutWF+qlQ\nXsvq6MvA47nfy057U3g1uI6OzZbTQbk0uwJd5eqkxjLXttjml860euf1xArfRRyZvRE4BVgMvI2N\nr2vNIM5qD8m+LwC+YGZnmtnejSqUu/8R+FtWloJTgLvc/fns+wSiOfMfZtaRa7a8CxiIDwf9KfAk\n8KUywyYAtwErcstqJbH8Grms/pCrHwDMbEszO9/M/g68Qqxr3wT2yDc1DxB9raODgOfcfV5hJHd/\nmmKzPtDYZa5tsXkUtHrnJWJDOBDYjTjquhnYAXgN8HxJ+sL3Idn7vwGzgK8Dj5jZo2b2/gaVbQbw\nXgvbAu8ia47I7AAcTDHoFl4fAXZvUBmS4e5rgYuAD5nZ8JLBOxA7mtJldSSNXVal6wvAhURTVCfR\nVHUQ8I1s2JYNnHa/14A62lomcLsAAAMISURBVInynSRKf2v0Mte22AQD7YitUda6+/wyvy8lVrph\nJb/vmL0vA3D35USb+tlmth/RfHiVmf3J3R/uY9lmAF8jrpntQRyY3JAbvgyYT1xDKfVKH6edqh8B\nX2Xj60XLiGshF5QZZ2X2viZ737xk+OuI9aEe5a5nvhe41N0vKvxgZsfWmd+mqC919BwwtMzwoRTr\nDxq/zLUtNoGCVgO5+zoz+wOx8l+WG/Q+YD1wT5lx/mRmXwA+COwDlAtar2bvNY/23P0hM3uQOPrc\nA7jd3V/IJbkDmAg86e6La8/Vps/dXzGzbwH/h2iu6coG3UHU3UPuvrrC6IVbE95MXKjHzHYn6vLR\nXLpX6dnR+lbkdlxmNgho1Nl4cvpYR/OA88xsrLvfB2BmuwIHAL/NpatnmWtbbDMFrcY7D/i1mV1J\nNAXsSxwF/tDj/iuyLq8ziQ4ZDnwC+CdwX4U8H8neP2lmPwdWufufq5RhBvAZYLss77yfEL2s5mY7\ngceJa3RjiXb/7/RgXjclPyAuuh9KXFOA6OX5IeBOM7uUuGdoR+AI4DfufrW7P2Vm84ELzGwVcTT9\nZbKz6py/AieY2SQi0D3j7s9UKc9twFnZ9ZVlwFnAFg2Yz5T1qo6AXwEPANeY2ZeIji/nEc2y63P5\n17PMtS22W7t7gqT2oqRnX4U0pwB/Jo7KniIu5nbkhl+cDV8JLCd6Hx5WbRrA54CFRO+jJ7LfJpPr\nsZRLu1f2+xpguzLl2w64BFiUK+MNwDvavXzbWYcUbz2Ym/ttF+BKYgf3CvAE0TFgVMnynksceDxC\n3Oowl+69B3cgDlSWZdOYlv3eLV0u/Y5Z+hXZtC8idnr5HmrjGQC9BxtUR8OBW7JtYiEwBbgVmNWT\nZZ6l07bYxpf+mkREBhwz2444s/meu5/X7vJI/dQ8KCKbPDM7g2gKfJTogPFZounvR+0sl/ScgpaI\nDARriJ6Hw4nmuvuACe6+sK2lkh5T86CIiCRDNxeLiEgyFLRERCQZCloiIpIMBS0REUmGgpaIiCTj\nfwHVSP2PSOEyGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot the final data using seaborns countplot to show distribution of sentiment in dataset\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(Final_data.sentiment, palette='viridis')\n",
    "g.set_xticklabels(['Positive', 'Neutral','Negative'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "# function to show values on bars\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "show_values_on_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Transactions', fontsize=30)\n",
    "plt.tick_params(axis='x', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "P91gR7xkFG0S",
    "outputId": "0d655e0c-21fd-40d1-ec45-9eb3b7f41f3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8805</th>\n",
       "      <td>buy think would great read book play game howe...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>good tablet kid lot appts download game</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>item work expect great product</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10143</th>\n",
       "      <td>great beginner like child limit use many apps ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>buy kindle past time one come defective port b...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Processed_Review sentiment\n",
       "8805   buy think would great read book play game howe...   Neutral\n",
       "9736             good tablet kid lot appts download game   Neutral\n",
       "125                       item work expect great product  Positive\n",
       "10143  great beginner like child limit use many apps ...   Neutral\n",
       "10937  buy kindle past time one come defective port b...   Neutral"
      ]
     },
     "execution_count": 205,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Final_data.sample(frac=0.1, random_state=0) #uncomment to use full set of data\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
    "#df['Sentiment'] = np.where(df['Sentiment'] > 3, 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urET5UsasCxf"
   },
   "outputs": [],
   "source": [
    "# Convert the sentiments\n",
    "# Positive=1,Negative=0,Neutral=2\n",
    "#df.sentiment.replace(('Positive','Negative','Neutral'),(1,0,2),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVIRj6lc1Bgy"
   },
   "source": [
    "# Train and Test Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Un8Fj0qFFISd",
    "outputId": "923c7fa7-d6d5-4846-e94e-d1d4a82e4304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 1012 training examples and 113 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " daughter love easy navigate hard break\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Processed_Review'], df['sentiment'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fp8AVYe41aZr"
   },
   "source": [
    "#Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyGMN7Q11V7h"
   },
   "source": [
    "The goal of this project is to classify the reviews into positive and negative sentiment. There are two main steps involved. First, we need to find a word embedding to convert a text into a numerical representation. Second, we fit the numerical representations of text to machine learning algorithms or deep learning architectures.\n",
    "\n",
    "One common approach of word embedding is frequency based embedding such as Bag of Words (BoW) model. BoW model learns a vocubulary list from a given corpus and represents each document based on some counting methods of words. In this part, we will explore the model performance of using BoW with supervised learning algorithms. Here's the workflow in this part.\n",
    "\n",
    "* Step 1 : Preprocess raw reviews to cleaned reviews\n",
    "* Step 2 : Create BoW using CountVectorizer / Tfidfvectorizer in sklearn\n",
    "* Step 3 : Transform review text to numerical representations (feature vectors)\n",
    "* Step 4 : Fit feature vectors to supervised learning algorithm (eg. Naive Bayes, Logistic regression, etc.)\n",
    "* Step 5 : Improve the model performance by GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crhrg17z33w7"
   },
   "source": [
    "#Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCB9DYy14B5I"
   },
   "source": [
    "The following text preprocessing are implemented to convert raw reviews to cleaned review, so that it will be easier for us to do feature extraction in the next step.\n",
    "* remove html tags using BeautifulSoup\n",
    "* remove non-character such as digits and symbols\n",
    "* convert to lower case\n",
    "* remove stop words such as \"the\" and \"and\" if needed\n",
    "* convert to root words by stemming if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCkNdIAoFJEA"
   },
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nSMrW5WAFJf0",
    "outputId": "1666a98d-428a-485b-b466-d1af837b5e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " daughter love easy navigate hard break\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text data in training set and validation set\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jzUZ8zH5UfA"
   },
   "source": [
    "#CountVectorizer with Mulinomial Naive Bayes (Benchmark Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05yHgKgW5YMa"
   },
   "source": [
    "Now we have cleaned reviews, the next step is to convert the reviews into numerical representations for machine learning algorithm.\n",
    "\n",
    "In sklearn library, we can use CountVectorizer which implements both tokenization and occurrence counting in a single class. The output is a sparse matrix representation of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "cxccfcR8FJ0Q",
    "outputId": "8d50950e-da61-42cb-fa08-6b3fb09c0346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 1511 \n",
      "\n",
      "Show some feature names : \n",
      " ['ability', 'playtime']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 210,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the training data to a document-term matrix using CountVectorizer\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
    "\n",
    "\n",
    "# Train MultinomialNB classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rnz_yIJ0FKBd"
   },
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    #print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "JYodGK4j1Nzh",
    "outputId": "d96496ba-a407-46cf-ce56-50bbee5299bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8938\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.95      0.94        39\n",
      "     Neutral       0.85      0.90      0.88        39\n",
      "    Positive       0.91      0.83      0.87        35\n",
      "\n",
      "    accuracy                           0.89       113\n",
      "   macro avg       0.89      0.89      0.89       113\n",
      "weighted avg       0.89      0.89      0.89       113\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[37  0  2]\n",
      " [ 3 35  1]\n",
      " [ 0  6 29]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validaton set\n",
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbxNJYWV67Ss"
   },
   "source": [
    "#TfidfVectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XxBnenb7A5z"
   },
   "source": [
    "Some words might frequently appear but have little meaningful information about the sentiment of a particular review. Instead of using occurance counting, we can use tf-idf transform to scale down the impact of frequently appeared words in a given corpus.\n",
    "\n",
    "In sklearn library, we can use TfidfVectorizer which implements both tokenization and tf-idf weighted counting in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "lOL_N_6t5rta",
    "outputId": "ded30117-399f-4404-cabd-c65a3d44cd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 691 \n",
      "\n",
      "Show some feature names : \n",
      " ['able']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 213,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9H4pWDCL5sDR",
    "outputId": "f178ef3d-3548-44c3-fb9f-f045022fa77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['love' 'easy' 'great' 'play' 'read' 'alexa' 'kid' 'price' 'well' 'enjoy']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['return' 'update' 'bad' 'know' 'terrible' 'th' 'poor' 'try' 'minute'\n",
      " 'youtube']\n"
     ]
    }
   ],
   "source": [
    "# Look at the top 10 features with smallest and the largest coefficients\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "pEQj53Sd5sWA",
    "outputId": "e45c2807-d76e-495b-b078-e464affcd4e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9292\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      1.00      0.96        39\n",
      "     Neutral       0.88      0.92      0.90        39\n",
      "    Positive       1.00      0.86      0.92        35\n",
      "\n",
      "    accuracy                           0.93       113\n",
      "   macro avg       0.94      0.93      0.93       113\n",
      "weighted avg       0.93      0.93      0.93       113\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[39  0  0]\n",
      " [ 3 36  0]\n",
      " [ 0  5 30]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the validaton set\n",
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAn_jdPT74-8"
   },
   "source": [
    "#Pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yctLoi8B77qN"
   },
   "source": [
    "In sklearn library, we can build a pipeline to streamline the workflow and use GridSearch on the pipeline model to implement hyper-parameter tuning for both vectorizer and classifier in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "sJPkRnc35som",
    "outputId": "b04c799c-859b-437e-92a6-a0404917a5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best paramenter set is : \n",
      " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "\n",
      "Accuracy on validation set: 0.9381\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.97      0.97        39\n",
      "     Neutral       0.90      0.95      0.92        39\n",
      "    Positive       0.94      0.89      0.91        35\n",
      "\n",
      "    accuracy                           0.94       113\n",
      "   macro avg       0.94      0.94      0.94       113\n",
      "weighted avg       0.94      0.94      0.94       113\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[38  0  1]\n",
      " [ 1 37  1]\n",
      " [ 0  4 31]]\n"
     ]
    }
   ],
   "source": [
    "# Building a pipeline\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "\n",
    "# Grid search\n",
    "params = {\"lr__C\":[0.1, 1, 10], #regularization param of logistic regression\n",
    "          \"tfidf__min_df\": [1, 3], #min count of words \n",
    "          \"tfidf__max_features\": [1000, None], #max features\n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tfidf__stop_words\": [None, \"english\"]} #use stopwords or don't\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
    "\n",
    "\n",
    "# Evaluate on the validaton set\n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCJYnw8-8TtR"
   },
   "source": [
    "#Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7IfsExR8Wv9"
   },
   "source": [
    "Another common approach of word embedding is prediction based embedding, such as Word2Vec model. In gist, Word2Vec is a combination of two techniques: Continuous Bag of Words (CBoW) and skip-gram model. Both are shallow neural networks which learn weights for word vector representations.\n",
    "\n",
    "In this part, we will train Word2Vec model to create our own word vector representations using gensim library. Then we fit the feature vectors of the reviews to Random Forest Classifier. Here's the workflow of this part.\n",
    "\n",
    "* Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)\n",
    "* Step 2 : Create volcabulary list using Word2Vec model\n",
    "* Step 3 : Transform each review into numerical representation by computing average feature vectors of words therein\n",
    "* Step 4 : Fit the average feature vectors to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0v9y-KT8p5e"
   },
   "source": [
    "#Parsing Review into Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN27mNNY8sUU"
   },
   "source": [
    "Word2Vec model takes a list of sentences as inputs and outputs word vector representations for words in the vocalbulary list created. Before we train the Word2Vec model, we have to parse reviews in the training set into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zsZuxQXW9uXL",
    "outputId": "4efa8a64-1f3d-4517-bbbd-630eaa3becfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "RGrS1cxU5tCk",
    "outputId": "b648855c-7bfd-4513-a60a-bf06f2e59265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['daughter', 'love', 'easy', 'navigate', 'hard', 'break']\n"
     ]
    }
   ],
   "source": [
    "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
    "# nltk.download()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "    \n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyEPpLnF_2P4"
   },
   "source": [
    "#Creating Volcabulary List usinhg Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87Dw4U9o_871"
   },
   "source": [
    "Now we have a set of cleaned and parsed sentences from the training data, we can train our own word evctor representations by sepcifiying the embedding dimension (= length of feature vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "5SotIQl35tWs",
    "outputId": "3f4dbeb7-495a-4ff6-bc95-4c845bb69d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 416 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['buy', 'tablet', 'use', 'good', 'great', 'work', 'get', 'one', 'amazon', 'kindle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Fit parsed sentences to Word2Vec model \n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300  #embedding dimension                     \n",
    "min_word_count = 10                \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-njoKC4AGyb"
   },
   "source": [
    "#Averaging Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqYGqZ1AAJ96"
   },
   "source": [
    "Now we have created a volcabulary list of words, with each word having a word representation (ie. feature vector of dim 300).\n",
    "\n",
    "To find a numerical representation for a review, we run through each word in a review text. For words appear in the volcabulary list, we compute the average feature vectors of all those words. The average feature vector is the numerical represenation of the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1ixUFAH5s5U"
   },
   "outputs": [],
   "source": [
    "# Transfrom the training data into feature vectors\n",
    "\n",
    "def makeFeatureVec(review, model, num_features):\n",
    "    '''\n",
    "    Transform a review to a feature vector by averaging feature vectors of words \n",
    "    appeared in that review and in the volcabulary list created\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "PxJkq37s1OAV",
    "outputId": "f1baff86-fcde-414e-a5d2-98d5fbee7402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 1012 feature vectors with 300 dimensions\n",
      "Validation set : 113 feature vectors with 300 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Get feature vectors for training set\n",
    "X_train_cleaned = []\n",
    "for review in X_train:\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
    "\n",
    "\n",
    "# Get feature vectors for validation set\n",
    "X_test_cleaned = []\n",
    "for review in X_test:\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)\n",
    "\n",
    "\n",
    "# debugging\n",
    "# print(\"Checkinf for NaN and Inf\")\n",
    "# print(\"np.inf=\", np.where(np.isnan(trainVector)))\n",
    "# print(\"is.inf=\", np.where(np.isinf(trainVector)))\n",
    "# print(\"np.max=\", np.max(abs(trainVector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVNq8WVWAaCT"
   },
   "source": [
    "#Random Forest Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxpTqksKAczr"
   },
   "source": [
    "We can now train Random Forest Classifier using feature vectors of reviews in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "SA3_SdZq1Nrt",
    "outputId": "13e102e2-323c-4a37-8f77-7fb3648e4737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9646\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.97      0.99        39\n",
      "     Neutral       0.95      0.97      0.96        39\n",
      "    Positive       0.94      0.94      0.94        35\n",
      "\n",
      "    accuracy                           0.96       113\n",
      "   macro avg       0.96      0.96      0.96       113\n",
      "weighted avg       0.97      0.96      0.96       113\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[38  0  1]\n",
      " [ 0 38  1]\n",
      " [ 0  2 33]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(trainVector, y_train)\n",
    "predictions = rf.predict(testVector)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wx46Yd9Aw2a"
   },
   "source": [
    "# Apply LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwiOmR7tA1a2"
   },
   "source": [
    "Long Short Term Memory networks (LSTM) are a special kind of Recurrent Neural Networks (RNN), capable of learning long-term dependencies. LSTM can be very usefull in text mining problems since it involves dependencies in the sentences which can be caught in the \"memory\" of the LSTM.\n",
    "\n",
    "In this part, we train a simple LSTM and a LSTM with Word2Vec embedding to classify the reviews into positive and negative  sentiment using Keras libarary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BF0magBPA_pM"
   },
   "source": [
    "#Simple LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oQjU0PgBIDj"
   },
   "source": [
    "We need to preprocess the text data to 2D tensor before we fit into a simple LSTM. First, we tokenize the corpus by only considering top words (top_words = 20000), and transform reviews to numerical sequences using the trained tokenizer. Next, we make sure that all numerical sequences have the same length (maxlen=100) for modeling, by truncating long reviews and pad shorter reviews with zero values.\n",
    "\n",
    "To construct a simple LSTM, we use embedding class in Keras to construct the first layer. This embedding layer converts numerical sequence of words into a word embedding. We should note that the embedding class provides a convenient way to map discrete words into a continuous vector space, but it does not take the semantic similarity of the words into account. The next layer is the LSTM layer with 128 memory units. Finally, we use a dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (positive sentiment and negative sentiment). Since it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). ADAM optimization algorithm is used.\n",
    "\n",
    "Here's the workflow in this part.\n",
    "* Step 1 : Prepare X_train and X_test to 2D tensor\n",
    "* Step 2 : Train a simple LSTM (embeddign layer => LSTM layer => dense layer)\n",
    "* Step 3 : Compile and fit the model using log loss function and ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiNBvLeOwlYv"
   },
   "outputs": [],
   "source": [
    "df = Final_data.sample(frac=0.1, random_state=0) #uncomment to use full set of data\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
    "df.head()\n",
    "\n",
    "# Convert the sentiments\n",
    "# Positive=1,Negative=0,Neutral=2\n",
    "df.sentiment.replace(('Positive','Negative','Neutral'),(1,0,2),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M15lkbD7wl1V"
   },
   "outputs": [],
   "source": [
    "# Split data into training set and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Processed_Review'], df['sentiment'], \\\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKPZnjT0wh5L"
   },
   "outputs": [],
   "source": [
    "#df.sentiment.replace(('Positive','Negative','Neutral'),(1,0,2),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Vsx8pyhFAnQ4",
    "outputId": "f8f4c7cb-dba4-413a-b2e6-05fc0c74551f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1012, 100)\n",
      "X_test shape: (113, 100)\n",
      "y_train shape: (1012, 3)\n",
      "y_test shape: (113, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "top_words = 20000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "nb_epoch = 3\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape) #(27799, 100)\n",
    "print('X_test shape:', X_test_seq.shape) #(3089, 100)\n",
    "print('y_train shape:', y_train_seq.shape) #(27799, 2)\n",
    "print('y_test shape:', y_test_seq.shape) #(3089, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "7Kt0CenRAn_v",
    "outputId": "c859b7e0-7837-4bdd-9216-d4b54ad858c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,691,971\n",
      "Trainable params: 2,691,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1012/1012 [==============================] - 8s 8ms/step - loss: 0.6179 - acc: 0.6673\n",
      "Epoch 2/3\n",
      "1012/1012 [==============================] - 7s 7ms/step - loss: 0.4082 - acc: 0.8109\n",
      "Epoch 3/3\n",
      "1012/1012 [==============================] - 6s 6ms/step - loss: 0.2093 - acc: 0.9318\n",
      "113/113 [==============================] - 0s 3ms/step\n",
      "Test loss : 0.2240\n",
      "Test accuracy : 0.8938\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple LSTM\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128, dropout=0.2))\n",
    "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "# Compile LSTM\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "# Model evluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "utPxtfAzAo2l",
    "outputId": "2721e9a6-6666-4e4b-d365-c4dfe37c4a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (20000, 128)\n",
      "Size of weight matrix in the hidden layer :  (128, 512)\n",
      "Size of weight matrix in the output layer :  (128, 3)\n"
     ]
    }
   ],
   "source": [
    "# get weight matrix of the embedding layer\n",
    "model1.layers[0].get_weights()[0] # weight matrix of the embedding layer, word-by-dim matrix\n",
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model1.layers[0].get_weights()[0].shape) #(20000, 128)\n",
    "\n",
    "# get weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model1.layers[1].get_weights()[0].shape) #(128, 512)  weight dim of LSTM - w\n",
    "\n",
    "# get weight matrix of the output layer\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model1.layers[2].get_weights()[0].shape) #(128, 2) weight dim of dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbOnHBxaFeOo"
   },
   "source": [
    "#LSTM with Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "741yG0xwFkQd"
   },
   "source": [
    "In the simple LSTM model constructed above, the embedding class in Keras comes in handy to converts numerical sequence of words into a word embedding, but it does not take the semantic similarity of the words into account.  The model assigns random weights to the embedding layer and learn the embeddings by minimizing the global error of the network.\n",
    "\n",
    "Instead of using random weights, we can use pretrained word embeddings to initialize the weight of an embedding layer. In this part, we use the Word2Vec embedding trained in Part 4 to \n",
    "intialize the weights of embedding layer in LSTM.\n",
    "\n",
    "* Step 1 : Load pretrained word embedding model\n",
    "* Step 2 : Construct embedding layer using embedding matrix as weights\n",
    "* Step 3 : Train a LSTM with Word2Vec embedding (embeddign layer => LSTM layer => dense layer)\n",
    "* Step 4 : Compile and fit the model using log loss function and ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Y5OXHsVtApHu",
    "outputId": "a8f4bb23-60a1-4508-fc41-61d82bc6ebe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix :  (416, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Load trained Word2Vec model\n",
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
    "\n",
    "\n",
    "# Get Word2Vec embedding matrix\n",
    "embedding_matrix = w2v.wv.syn0  # embedding matrix, type = numpy.ndarray \n",
    "print(\"Shape of embedding matrix : \", embedding_matrix.shape) #(4016, 300) = (volcabulary size, embedding dimension)\n",
    "# w2v.wv.syn0[0] #feature vector of the first word in the volcabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ZOpIia0YApfX",
    "outputId": "330869f1-e455-48b7-e404-e66d03a14fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1012, 100)\n",
      "X_test shape: (113, 100)\n",
      "y_train shape: (1012, 3)\n",
      "y_test shape: (113, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "top_words = embedding_matrix.shape[0] #4016\n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "nb_epoch = 3\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape) #(27799, 100)\n",
    "print('X_test shape:', X_test_seq.shape) #(3089, 100)\n",
    "print('y_train shape:', y_train_seq.shape) #(27799, 2)\n",
    "print('y_test shape:', y_test_seq.shape) #(3089, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "YDduM-P5Ap3g",
    "outputId": "265b9f5d-bc99-47dc-ae7b-b060759c1d60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 300)         124800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 344,835\n",
      "Trainable params: 344,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1012/1012 [==============================] - 9s 9ms/step - loss: 0.6253 - acc: 0.6667\n",
      "Epoch 2/3\n",
      "1012/1012 [==============================] - 8s 7ms/step - loss: 0.4611 - acc: 0.7800\n",
      "Epoch 3/3\n",
      "1012/1012 [==============================] - 8s 8ms/step - loss: 0.2628 - acc: 0.8949\n",
      "113/113 [==============================] - 0s 4ms/step\n",
      "Test loss : 0.2882\n",
      "Test accuracy : 0.8525\n"
     ]
    }
   ],
   "source": [
    "# Construct Word2Vec embedding layer\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
    "                            embedding_matrix.shape[1], #300\n",
    "                            weights=[embedding_matrix])\n",
    "\n",
    "\n",
    "# Construct LSTM with Word2Vec embedding\n",
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "BdNAVhnyAqKe",
    "outputId": "44fab240-e5a3-4324-c4ed-05498ede6535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (416, 300)\n",
      "Size of weight matrix in the hidden layer :  (300, 512)\n",
      "Size of weight matrix in the output layer :  (128, 3)\n"
     ]
    }
   ],
   "source": [
    "# get weight matrix of the embedding layer\n",
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model2.layers[0].get_weights()[0].shape) #(20000, 128)\n",
    "\n",
    "# get weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model2.layers[1].get_weights()[0].shape) #(128, 512)  weight dim of LSTM - w\n",
    "\n",
    "# get weight matrix of the output layer\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model2.layers[2].get_weights()[0].shape) #(128, 2) weight dim of dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhywU3s9Aoka"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone Retail Amazon with LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
